{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "npl",
   "display_name": "Python (npl)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/claudiobarril/pln1_17co2024/blob/main/Desafio_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "metadata": {
    "id": "F3CNbPztnmzU"
   },
   "cell_type": "markdown",
   "source": [
    "## Desafío 2: Custom embedddings con Gensim"
   ]
  },
  {
   "metadata": {
    "id": "Okf_AjJxnmzW"
   },
   "cell_type": "markdown",
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizarán los cuentos de \"Las mil y una noches\" para generar los embeddings, es decir, que los vectores tendrán la forma en función de como se hayan utilizado las palabras en dichas historias."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pexls2458ThN",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:40.253350Z",
     "start_time": "2025-04-03T03:21:36.466063Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import requests\n",
    "import fitz\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import fitz\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "fK5j69x7nmzX"
   },
   "cell_type": "markdown",
   "source": [
    "Convertir PDF descargado de https://www.textos.info/anonimo/las-mil-y-una-noches a TXT"
   ]
  },
  {
   "metadata": {
    "id": "NboviY6BnmzX",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:46.070085Z",
     "start_time": "2025-04-03T03:21:40.254504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])  # Extraer texto de cada página\n",
    "    return text\n",
    "\n",
    "# Cargar y guardar el texto\n",
    "pdf_text = pdf_to_text(\"desafio_2/Anonimo - Las Mil y Una Noches.pdf\")\n",
    "with open(\"desafio_2/las-mil-y-una-noches.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(pdf_text)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "IzUoEnQjnmzX"
   },
   "cell_type": "markdown",
   "source": [
    "#### Construcción del corpus\n",
    "\n",
    "Vamos a probar dos estratégias distintas. Crear un documento por cada línea del libro, y un documento por cada página.\n",
    "Sería interesante probar un documento por cuento, pero la separación del libro en cuentos no es trivial, y debería utilizarse una lista de todos los cuentos para dicha tarea. Se abordó esa idea sin éxito en un tiempo prudente, por lo que fue descartada.\n",
    "\n",
    "Como lado positivo, vamos a tener documentos cortos y contexto local, aunque como contra punto, puede que dicho contexto local se pierda en varias situaciones, dado que cortamos indiscriminadamente cuentos por la mitad, ya sea por línea o por página."
   ]
  },
  {
   "metadata": {
    "id": "aXxZ9wn4nmzX"
   },
   "cell_type": "markdown",
   "source": [
    "##### Un documento por página"
   ]
  },
  {
   "metadata": {
    "id": "DS5znWS4nmzX",
    "outputId": "af1efd70-9c4e-4583-dc2b-96584736c2c4",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:46.137414Z",
     "start_time": "2025-04-03T03:21:46.070899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Leer el archivo .txt completo\n",
    "with open(\"desafio_2/las-mil-y-una-noches.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Dividir por páginas usando saltos dobles de línea\n",
    "pages = text.split(\"\\n\\n\")\n",
    "\n",
    "# Filtrar líneas que son solo números (números de página)\n",
    "clean_pages = [re.sub(r\"^\\d+$\", \"\", page, flags=re.MULTILINE).strip() for page in pages]\n",
    "\n",
    "# Eliminar entradas vacías después de la limpieza\n",
    "clean_pages = [page for page in clean_pages if page]\n",
    "\n",
    "# Quitar las primeras 6 páginas (introducción)\n",
    "clean_pages = clean_pages[6:]\n",
    "\n",
    "# Crear un DataFrame con cada página limpia como un documento\n",
    "df_by_pages = pd.DataFrame(clean_pages, columns=[\"text\"])\n",
    "\n",
    "# Mostrar algunas páginas procesadas\n",
    "for i, page in enumerate(df_by_pages[\"text\"].head(7)):\n",
    "    print(f\"\\n--- Página {i+1} ---\\n\")\n",
    "    print(page[:500])  # Mostrar los primeros 500 caracteres de cada fragmento\n",
    "\n",
    "print(\"\\nCantidad de documentos:\", df_by_pages.shape[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Página 1 ---\n",
      "\n",
      "Historia del rey Schahriar y su hermano el rey \n",
      "Schahzaman\n",
      "Cuéntase —pero Alah es más sabio, más prudente más poderoso y más \n",
      "benéfico— que en lo que transcurrió en la antigüedad del tiempo y en lo \n",
      "pasado de la edad, hubo un rey entre los reyes de Sassan, en las islas de \n",
      "la India y de la China. Era dueño de ejércitos y señor de auxiliares, de \n",
      "servidores y de un séquito numeroso. Tenía dos hijos, y ambos eran \n",
      "heroicos jinetes, pero el mayor valía más aún que el menor. El mayor reinó \n",
      "en los p\n",
      "\n",
      "--- Página 2 ---\n",
      "\n",
      "hermano?» Desenvainó inmediatamente su alfanje, y acometiendo a \n",
      "ambos, los dejó muertos sobre los tapices del lecho. Volvió a salir sin \n",
      "perder una hora ni un instante, y ordenó la marcha de la comitiva. Y viajó \n",
      "de noche, hasta avistar la ciudad de su hermano.\n",
      "Entonces éste se alegró de su proximidad, salió a su encuentro, y al \n",
      "recibirlo, le deseó la paz. Se regocijó hasta los mayores límites del \n",
      "contento, mandó adornar en honor suyo la ciudad, y se puso a hablarle \n",
      "lleno de efusión. Pero el\n",
      "\n",
      "--- Página 3 ---\n",
      "\n",
      "toda su alma después de haberse alimentado parcamente en los primeros \n",
      "días. Se asombró de ello, y dijo: «Hermano, poco ha te veía amarillo de tez \n",
      "y ahora has recuperado los colores. Cuéntame qué te pasa». El rey le dijo: \n",
      "«Te contaré la causa de mi anterior palidez, pero dispénsame de referirte \n",
      "el motivo de haber recobrado los colores». El rey replicó: «Para \n",
      "entendernos, relata primeramente la causa de tu pérdida de color y tu \n",
      "debilidad». Y se explicó de este modo: «Sabrás hermano, que cuan\n",
      "\n",
      "--- Página 4 ---\n",
      "\n",
      "mar salada. En aquella pradera había un manantial de agua dulce. \n",
      "Bebieron de ella y se sentaron a descansar.\n",
      "Apenas había transcurrido una hora del día, cuando el mar empezó a \n",
      "agitarse. De pronto brotó de él una negra columna de humo, que llegó \n",
      "hasta el cielo y se dirigió después hacia la pradera. Los reyes, asustados, \n",
      "se subieron a la cima del árbol, que era muy alto, y se pusieron a mirar lo \n",
      "que tal cosa pudiera ser. Y he aquí que la columna de humo se convirtió \n",
      "en un efrit de elevada es\n",
      "\n",
      "--- Página 5 ---\n",
      "\n",
      "ojos? Si no venís y me obedecéis, llamo inmediatamente al efrit». \n",
      "Entonces, por miedo al efrit hicieron con ella lo que les había pedido. \n",
      "Cuando los hubo agotado, les dijo: «¡Qué expertos sois los dos!» Sacó del \n",
      "bolsillo un saquito y del saquito un collar compuesto de quinientas setenta \n",
      "sortijas con sellos, y les preguntó: «¿Sabéis lo que es esto?» Ellos \n",
      "contestaron: «No lo sabemos». Entonces les explicó la joven: «Los dueños \n",
      "de estos anillos me han poseído todos junto a los cuernos insens\n",
      "\n",
      "--- Página 6 ---\n",
      "\n",
      "para los asaltos de este cabalgador.\n",
      "En esta situación, el rey mandó al visir que, como de costumbre, le trajese \n",
      "una joven. El visir, por más que buscó, no pudo encontrar ninguna, y \n",
      "regresó muy triste a su casa, con el alma transida de miedo ante el furor \n",
      "del rey. Pero este visir tenía dos hijas de gran hermosura, que poseían \n",
      "todos los encantos, todas las perfecciones y eran de una delicadeza \n",
      "exquisita. La mayor se llamaba Schehrazada , y el nombre de la menor era \n",
      "Doniazada.\n",
      "La mayor, Sche\n",
      "\n",
      "--- Página 7 ---\n",
      "\n",
      "Fábulas del asno, el buey y el labrador\n",
      "«Has de saber, hija mía, que hubo un comerciante dueño de grandes \n",
      "riquezas y de mucho ganado. Estaba casado y con hijos. Alah, el Altísimo, \n",
      "le dio igualmente el conocimiento de los lenguajes de los animales y el \n",
      "canto de los pájaros. Habitaba este comerciante en un país fértil, a orillas \n",
      "de un río. En su morada había un asno y un buey.\n",
      "»Cierto día llegó el buey al lugar ocupado por el asno y vio aquel sitio \n",
      "barrido y regado. En el pesebre había cebada\n",
      "\n",
      "Cantidad de documentos: 3626\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "ve0QQEoEnmzY",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:46.771919Z",
     "start_time": "2025-04-03T03:21:46.138673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence_tokens_by_pages = [text_to_word_sequence(page) for page in clean_pages]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 - Crear los vectores (word2vec)"
   ],
   "metadata": {
    "id": "Picq7M-8Fa_O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Creamos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model_by_pages = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=300,# dimensionalidad de los vectores\n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=4,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram\n",
    "\n",
    "# Obtener el vocabulario con los tokens\n",
    "w2v_model_by_pages.build_vocab(sentence_tokens_by_pages)\n",
    "\n",
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model_by_pages.corpus_count)\n",
    "\n",
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model_by_pages.wv.index_to_key))"
   ],
   "metadata": {
    "id": "oiacqmNfFeff",
    "outputId": "1e32c322-c57f-4673-a817-2ba2e2631871",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:46.979855Z",
     "start_time": "2025-04-03T03:21:46.772676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 3626\n",
      "Cantidad de words distintas en el corpus: 12882\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "Cjt2BCiSnmzY"
   },
   "cell_type": "markdown",
   "source": [
    "##### Un documento por línea"
   ]
  },
  {
   "metadata": {
    "id": "elF_cMjTnmzY",
    "outputId": "fdf8e3da-02dc-4536-a176-b25f2d53b54f",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:47.041114Z",
     "start_time": "2025-04-03T03:21:46.980458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Leer el archivo .txt línea por línea\n",
    "with open(\"desafio_2/las-mil-y-una-noches.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Limpiar cada línea: quitar espacios extra y números de página\n",
    "clean_lines = [re.sub(r\"^\\d+$\", \"\", line.strip()) for line in lines]\n",
    "\n",
    "# Eliminar líneas vacías después de la limpieza\n",
    "clean_lines = [line for line in clean_lines if line]\n",
    "\n",
    "# Quitar las primeras 140 líneas (introducción)\n",
    "clean_lines = clean_lines[126:]\n",
    "\n",
    "# Crear DataFrame con cada línea como un documento\n",
    "df_by_lines = pd.DataFrame(clean_lines, columns=[\"text\"])\n",
    "\n",
    "# Mostrar algunas líneas procesadas\n",
    "for i, line in enumerate(df_by_lines[\"text\"].head(10)):\n",
    "    print(f\"Línea {i}: {line}\")  # Mostrar la línea completa\n",
    "\n",
    "print(\"\\nCantidad de documentos:\", df_by_lines.shape[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Línea 0: Historia del rey Schahriar y su hermano el rey\n",
      "Línea 1: Schahzaman\n",
      "Línea 2: Cuéntase —pero Alah es más sabio, más prudente más poderoso y más\n",
      "Línea 3: benéfico— que en lo que transcurrió en la antigüedad del tiempo y en lo\n",
      "Línea 4: pasado de la edad, hubo un rey entre los reyes de Sassan, en las islas de\n",
      "Línea 5: la India y de la China. Era dueño de ejércitos y señor de auxiliares, de\n",
      "Línea 6: servidores y de un séquito numeroso. Tenía dos hijos, y ambos eran\n",
      "Línea 7: heroicos jinetes, pero el mayor valía más aún que el menor. El mayor reinó\n",
      "Línea 8: en los países, gobernó con justicia entre los hombres y por eso le querían\n",
      "Línea 9: los habitantes del país y del reino. Llamábase el rey Schahriar. Su\n",
      "\n",
      "Cantidad de documentos: 105622\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "IZj4_BIenmzZ",
    "outputId": "cadebb94-4e11-4318-aa27-eec2af5a197c",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:48.092432Z",
     "start_time": "2025-04-03T03:21:47.041747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizar cada línea en palabras\n",
    "sentence_tokens_by_lines = [text_to_word_sequence(line) for line in clean_lines]\n",
    "\n",
    "# Definir y entrenar el modelo Word2Vec\n",
    "w2v_model_by_lines = Word2Vec(\n",
    "    min_count=5,    # Frecuencia mínima de palabra\n",
    "    window=2,       # Contexto a considerar (palabras antes y después)\n",
    "    vector_size=300,# Dimensión de los embeddings\n",
    "    negative=20,    # Negative sampling\n",
    "    workers=4,      # Número de núcleos para entrenamiento\n",
    "    sg=1            # Skip-gram (1) en lugar de CBOW (0)\n",
    ")\n",
    "\n",
    "# Construir vocabulario\n",
    "w2v_model_by_lines.build_vocab(sentence_tokens_by_lines)\n",
    "\n",
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model_by_lines.corpus_count)\n",
    "\n",
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model_by_lines.wv.index_to_key))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 105622\n",
      "Cantidad de words distintas en el corpus: 12882\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 - Entrenar embeddings"
   ],
   "metadata": {
    "id": "xbmNEgJTFnqH"
   }
  },
  {
   "metadata": {
    "id": "Ugfo9iW3nmzZ",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:21:48.096006Z",
     "start_time": "2025-04-03T03:21:48.093776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss - self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Entrenamos el modelo generador de vectores, por página\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model_by_pages.train(sentence_tokens_by_pages,\n",
    "    total_examples=w2v_model_by_pages.corpus_count,\n",
    "    epochs=50,\n",
    "    compute_loss = True,\n",
    "    callbacks=[callback()]\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoZGkDUhFl07",
    "outputId": "1eac5241-4b6c-410a-933f-1d13acaa43cf",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:24:43.987599Z",
     "start_time": "2025-04-03T03:21:48.704920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 2330315.5\n",
      "Loss after epoch 1: 1773089.5\n",
      "Loss after epoch 2: 1633400.0\n",
      "Loss after epoch 3: 1560597.5\n",
      "Loss after epoch 4: 1559887.5\n",
      "Loss after epoch 5: 1488508.0\n",
      "Loss after epoch 6: 1473227.0\n",
      "Loss after epoch 7: 1463504.0\n",
      "Loss after epoch 8: 1390066.0\n",
      "Loss after epoch 9: 1431258.0\n",
      "Loss after epoch 10: 1366935.0\n",
      "Loss after epoch 11: 1284492.0\n",
      "Loss after epoch 12: 1317642.0\n",
      "Loss after epoch 13: 1293910.0\n",
      "Loss after epoch 14: 1284408.0\n",
      "Loss after epoch 15: 1279894.0\n",
      "Loss after epoch 16: 1270174.0\n",
      "Loss after epoch 17: 1253938.0\n",
      "Loss after epoch 18: 1242790.0\n",
      "Loss after epoch 19: 1200412.0\n",
      "Loss after epoch 20: 1197480.0\n",
      "Loss after epoch 21: 1186100.0\n",
      "Loss after epoch 22: 1225894.0\n",
      "Loss after epoch 23: 1206434.0\n",
      "Loss after epoch 24: 1142708.0\n",
      "Loss after epoch 25: 1132376.0\n",
      "Loss after epoch 26: 1139512.0\n",
      "Loss after epoch 27: 1119308.0\n",
      "Loss after epoch 28: 1114292.0\n",
      "Loss after epoch 29: 1120916.0\n",
      "Loss after epoch 30: 1115304.0\n",
      "Loss after epoch 31: 1119956.0\n",
      "Loss after epoch 32: 1102792.0\n",
      "Loss after epoch 33: 1100024.0\n",
      "Loss after epoch 34: 1099360.0\n",
      "Loss after epoch 35: 1105372.0\n",
      "Loss after epoch 36: 1094364.0\n",
      "Loss after epoch 37: 1099908.0\n",
      "Loss after epoch 38: 1057396.0\n",
      "Loss after epoch 39: 1094204.0\n",
      "Loss after epoch 40: 1088900.0\n",
      "Loss after epoch 41: 1073264.0\n",
      "Loss after epoch 42: 1055088.0\n",
      "Loss after epoch 43: 1048372.0\n",
      "Loss after epoch 44: 1044524.0\n",
      "Loss after epoch 45: 1085968.0\n",
      "Loss after epoch 46: 1076036.0\n",
      "Loss after epoch 47: 1048896.0\n",
      "Loss after epoch 48: 1082008.0\n",
      "Loss after epoch 49: 1044916.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41137461, 61999700)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "id": "C0i2cvo4nmzZ",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:28:56.583913Z",
     "start_time": "2025-04-03T03:26:34.153683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Entrenamos el modelo generador de vectores, por línea\n",
    "w2v_model_by_lines.train(sentence_tokens_by_lines,\n",
    "    total_examples=w2v_model_by_lines.corpus_count,\n",
    "    epochs=50,\n",
    "    compute_loss = True,\n",
    "    callbacks=[callback()]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 1421178.5\n",
      "Loss after epoch 1: 1317841.75\n",
      "Loss after epoch 2: 1270071.25\n",
      "Loss after epoch 3: 1197717.5\n",
      "Loss after epoch 4: 1176864.5\n",
      "Loss after epoch 5: 1175219.5\n",
      "Loss after epoch 6: 1149962.0\n",
      "Loss after epoch 7: 1085725.0\n",
      "Loss after epoch 8: 1082795.0\n",
      "Loss after epoch 9: 1071037.0\n",
      "Loss after epoch 10: 1076365.0\n",
      "Loss after epoch 11: 1071650.0\n",
      "Loss after epoch 12: 1067725.0\n",
      "Loss after epoch 13: 1084415.0\n",
      "Loss after epoch 14: 1041135.0\n",
      "Loss after epoch 15: 968792.0\n",
      "Loss after epoch 16: 968064.0\n",
      "Loss after epoch 17: 991904.0\n",
      "Loss after epoch 18: 963262.0\n",
      "Loss after epoch 19: 963526.0\n",
      "Loss after epoch 20: 960658.0\n",
      "Loss after epoch 21: 957362.0\n",
      "Loss after epoch 22: 949006.0\n",
      "Loss after epoch 23: 970734.0\n",
      "Loss after epoch 24: 967886.0\n",
      "Loss after epoch 25: 941018.0\n",
      "Loss after epoch 26: 943678.0\n",
      "Loss after epoch 27: 964640.0\n",
      "Loss after epoch 28: 958978.0\n",
      "Loss after epoch 29: 934576.0\n",
      "Loss after epoch 30: 925122.0\n",
      "Loss after epoch 31: 946776.0\n",
      "Loss after epoch 32: 854892.0\n",
      "Loss after epoch 33: 852668.0\n",
      "Loss after epoch 34: 865176.0\n",
      "Loss after epoch 35: 870848.0\n",
      "Loss after epoch 36: 873052.0\n",
      "Loss after epoch 37: 847448.0\n",
      "Loss after epoch 38: 843640.0\n",
      "Loss after epoch 39: 840112.0\n",
      "Loss after epoch 40: 841996.0\n",
      "Loss after epoch 41: 861388.0\n",
      "Loss after epoch 42: 839912.0\n",
      "Loss after epoch 43: 837600.0\n",
      "Loss after epoch 44: 852400.0\n",
      "Loss after epoch 45: 834680.0\n",
      "Loss after epoch 46: 828276.0\n",
      "Loss after epoch 47: 835372.0\n",
      "Loss after epoch 48: 832088.0\n",
      "Loss after epoch 49: 834284.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41136931, 61999700)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# Guardamos los modelos\n",
    "with open('desafio_2/w2v_model_by_pages.pkl', 'wb') as file:\n",
    "    pickle.dump(w2v_model_by_pages, file)\n",
    "\n",
    "with open('desafio_2/w2v_model_by_lines.pkl', 'wb') as file:\n",
    "    pickle.dump(w2v_model_by_lines, file)"
   ],
   "metadata": {
    "id": "LDTBEee68rgC",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:29:11.096446Z",
     "start_time": "2025-04-03T03:29:11.012758Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "3s5btQXpnmzZ"
   },
   "cell_type": "markdown",
   "source": [
    "### 4 - Ensayar"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cargamos los modelos desde archivo (no es necesario si los entrenamos en esta corrida)\n",
    "with open('desafio_2/w2v_model_by_pages.pkl', 'rb') as file:\n",
    "    w2v_model_by_pages = pickle.load(file)\n",
    "\n",
    "with open('desafio_2/w2v_model_by_lines.pkl', 'rb') as file:\n",
    "    w2v_model_by_lines = pickle.load(file)"
   ],
   "metadata": {
    "id": "VVaNPcDGW0iJ",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:29:12.724385Z",
     "start_time": "2025-04-03T03:29:12.685455Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "id": "eUh_JAlknmzZ",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:29:30.522362Z",
     "start_time": "2025-04-03T03:29:30.425527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_models(word, model_by_pages, model_by_lines, topn=10):\n",
    "    # Obtener palabras más similares en ambos modelos\n",
    "    similar_by_pages = model_by_pages.wv.most_similar(positive=[word], topn=topn)\n",
    "    similar_by_lines = model_by_lines.wv.most_similar(positive=[word], topn=topn)\n",
    "\n",
    "    # Convertir a listas formateadas\n",
    "    words_pages = [f\"{w[0]} ({w[1]:.2f})\" for w in similar_by_pages]\n",
    "    words_lines = [f\"{w[0]} ({w[1]:.2f})\" for w in similar_by_lines]\n",
    "\n",
    "    # Asegurar que ambas listas tengan el mismo tamaño\n",
    "    max_len = max(len(words_pages), len(words_lines))\n",
    "    words_pages += [\"\"] * (max_len - len(words_pages))\n",
    "    words_lines += [\"\"] * (max_len - len(words_lines))\n",
    "\n",
    "    # Crear tabla formateada\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Comparación para la palabra: {word}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"   Model By Pages   |  Model By Lines \")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for w1, w2 in zip(words_pages, words_lines):\n",
    "        print(f\" {w1.ljust(18)} | {w2}\")\n",
    "\n",
    "    print(\"-\" * 50)  # Línea final separadora\n",
    "\n",
    "# Lista de palabras a comparar\n",
    "words = [\"lámpara\", \"sultán\", \"genio\", \"alfombra\", \"califa\", \"anillo\", \"príncipe\", \"emir\", \"mezquita\", \"palacio\", \"seda\", \"puñal\", \"incienso\", \"destino\"]\n",
    "\n",
    "# Generar comparación para cada palabra\n",
    "for word in words:\n",
    "    compare_models(word, w2v_model_by_pages, w2v_model_by_lines)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Comparación para la palabra: lámpara\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " mágica (0.42)      | mágica (0.42)\n",
      " redoma (0.34)      | genni (0.32)\n",
      " anunciarme (0.33)  | borrachera (0.31)\n",
      " cincelado (0.33)   | cacerola (0.31)\n",
      " misiva (0.32)      | suspendido (0.31)\n",
      " adquisición (0.32) | alegrará (0.30)\n",
      " gustosa (0.32)     | fijó (0.30)\n",
      " cortina (0.31)     | cincelado (0.30)\n",
      " sirves (0.31)      | curcusilla (0.30)\n",
      " alegrará (0.31)    | redoma (0.30)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: sultán\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " rey (0.37)         | schams (0.35)\n",
      " sabur (0.35)       | rey (0.34)\n",
      " rey» (0.34)        | sabur (0.34)\n",
      " zahr (0.34)        | zahr (0.34)\n",
      " baibars (0.34)     | schah (0.34)\n",
      " schams (0.33)      | kabul (0.33)\n",
      " visir (0.33)       | visir (0.33)\n",
      " embajador (0.33)   | saladino (0.33)\n",
      " mahmud (0.32)      | qamús (0.32)\n",
      " daula (0.32)       | gobernador (0.32)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: genio\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " sucio (0.46)       | gigantesco (0.45)\n",
      " gigantesco (0.44)  | linaje (0.41)\n",
      " charlatán (0.40)   | obediente (0.41)\n",
      " llegaría (0.40)    | sucio (0.40)\n",
      " prístina (0.39)    | íntegro (0.38)\n",
      " impotente (0.39)   | estéril (0.38)\n",
      " activo (0.38)      | activo (0.37)\n",
      " experto (0.38)     | otorgues (0.37)\n",
      " flaco (0.38)       | palomo (0.37)\n",
      " vicios (0.38)      | vasto (0.36)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: alfombra\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " fanega (0.36)      | rodilla (0.33)\n",
      " toalla (0.33)      | esperara (0.32)\n",
      " servilleta (0.33)  | varita (0.32)\n",
      " varita (0.33)      | volante (0.32)\n",
      " facultad (0.32)    | mondada (0.31)\n",
      " tarima (0.32)      | maza (0.31)\n",
      " verdeante (0.32)   | divanes (0.31)\n",
      " pradera (0.32)     | servilleta (0.30)\n",
      " extendía (0.31)    | sentarse (0.30)\n",
      " lugartenienta (0.31) | ceremoniosamente (0.30)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: califa\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " billah (0.43)      | billah (0.41)\n",
      " mosslim (0.42)     | ala'llah (0.41)\n",
      " montasser (0.41)   | mosslim (0.41)\n",
      " moinben (0.40)     | montasser (0.39)\n",
      " motawakkil (0.38)  | martín (0.37)\n",
      " ala'llah (0.37)    | moinben (0.36)\n",
      " rey» (0.37)        | barmaki (0.35)\n",
      " harún (0.36)       | harún (0.35)\n",
      " martín (0.36)      | emocionó (0.34)\n",
      " motazid (0.36)     | motazid (0.34)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: anillo\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " talismánico (0.49) | talismánico (0.43)\n",
      " engarce (0.45)     | engarce (0.41)\n",
      " mágico (0.41)      | brazalete (0.36)\n",
      " canuto (0.38)      | ¡tráeme (0.36)\n",
      " rebelde (0.37)     | meñique (0.36)\n",
      " frotar (0.37)      | líquido (0.34)\n",
      " manuscrito (0.37)  | dandana (0.33)\n",
      " brazalete (0.36)   | búcaro (0.33)\n",
      " genni (0.36)       | mágico (0.33)\n",
      " archivo (0.36)     | ajustó (0.33)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: príncipe\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " hossein (0.43)     | hossein (0.46)\n",
      " faruz (0.37)       | faruz (0.35)\n",
      " muluk (0.36)       | púsose (0.35)\n",
      " yamaní (0.34)      | rey (0.32)\n",
      " nurgihán (0.34)    | scharkán (0.32)\n",
      " farid (0.34)       | yamaní (0.32)\n",
      " púsose (0.33)      | schater (0.31)\n",
      " farah (0.32)       | cercioró (0.31)\n",
      " schaibar (0.32)    | mubarak (0.31)\n",
      " mubarak (0.32)     | síndico (0.31)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: emir\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " comendador (0.54)  | creyentes (0.51)\n",
      " creyentes (0.54)   | comendador (0.50)\n",
      " defensor (0.40)    | moinben (0.40)\n",
      " vicario (0.40)     | motawakkil (0.40)\n",
      " tapete (0.40)      | ala'llah (0.39)\n",
      " muza (0.39)        | dilatados (0.38)\n",
      " motawakkil (0.39)  | albahaca (0.38)\n",
      " abdossamad (0.39)  | abdossamad (0.37)\n",
      " moinben (0.38)     | muza (0.37)\n",
      " mosslim (0.38)     | vicario (0.37)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: mezquita\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " galerías (0.37)    | alquilé (0.35)\n",
      " viajamos (0.36)    | congregados (0.35)\n",
      " sesión (0.35)      | viajamos (0.35)\n",
      " gradas (0.34)      | gradas (0.33)\n",
      " vertiente (0.34)   | embarcación (0.33)\n",
      " habitación (0.34)  | arquitectos (0.32)\n",
      " allende (0.34)     | alojaba (0.32)\n",
      " congregados (0.33) | serur (0.32)\n",
      " embarcación (0.33) | pórtico (0.32)\n",
      " pórtico (0.33)     | khaitán (0.31)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: palacio\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " celda (0.39)       | celda (0.41)\n",
      " pabellón (0.38)    | harem (0.38)\n",
      " gabinete (0.37)    | maghreb (0.37)\n",
      " gennistán (0.37)   | pabellón (0.36)\n",
      " aparato (0.37)     | penetramos (0.36)\n",
      " cortijo (0.36)     | aposento (0.36)\n",
      " harem (0.36)       | aparato (0.34)\n",
      " aposento (0.36)    | sucederme (0.34)\n",
      " penetramos (0.36)  | maristán (0.34)\n",
      " edificio (0.36)    | cortijo (0.34)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: seda\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " borlas (0.50)      | borlas (0.47)\n",
      " muselina (0.48)    | tisú (0.45)\n",
      " tisú (0.46)        | muselina (0.44)\n",
      " chapas (0.46)      | afiligranado (0.44)\n",
      " cachemira (0.46)   | gualdrapa (0.42)\n",
      " afiligranado (0.45) | brocato (0.42)\n",
      " colgaduras (0.45)  | bordada (0.41)\n",
      " mosul (0.43)       | chapas (0.41)\n",
      " brocado (0.43)     | mosul (0.41)\n",
      " bordada (0.43)     | labrados (0.40)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: puñal\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " tamboril (0.43)    | afiligranado (0.39)\n",
      " afiligranado (0.42) | cuchillo (0.38)\n",
      " borlas (0.41)      | capote (0.37)\n",
      " blandía (0.39)     | cubriéndose (0.36)\n",
      " firmán (0.39)      | bolsillos (0.36)\n",
      " perfumarse (0.38)  | blandía (0.36)\n",
      " mostrador (0.38)   | desenrollé (0.35)\n",
      " majestuosamente (0.37) | chapas (0.35)\n",
      " dosel (0.37)       | índice (0.35)\n",
      " cinta (0.37)       | bordadas (0.35)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: incienso\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " benjuí (0.49)      | pebeteros (0.45)\n",
      " extracto (0.46)    | nadd (0.45)\n",
      " pebeteros (0.46)   | extracto (0.43)\n",
      " nadd (0.45)        | patriarcales (0.43)\n",
      " patriarcales (0.44) | benjuí (0.41)\n",
      " pastas (0.44)      | cidras (0.40)\n",
      " áloe (0.44)        | áloe (0.40)\n",
      " cidras (0.43)      | almizclada (0.39)\n",
      " almizclada (0.42)  | quemaron (0.39)\n",
      " gris (0.41)        | colocadas (0.37)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Comparación para la palabra: destino\n",
      "==================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "--------------------------------------------------\n",
      " cumplirse (0.33)   | cumplirse (0.33)\n",
      " todopoderoso (0.32) | memorable (0.32)\n",
      " entristece (0.32)  | aguijón (0.32)\n",
      " altísimo (0.32)    | alah (0.31)\n",
      " fatalidad (0.32)   | rihán (0.31)\n",
      " vana (0.32)        | lapidado (0.30)\n",
      " repliqué (0.31)    | criminales (0.30)\n",
      " manifestación (0.31) | protege (0.30)\n",
      " someterse (0.31)   | subsistencia (0.30)\n",
      " desgracias (0.31)  | libertinaje (0.30)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "FKaYs6CwnmzZ"
   },
   "cell_type": "markdown",
   "source": [
    "Podemos observar muchas palabras con asociaciones semánticamente relacionadas. Por ejemplo:\n",
    "* \"lámpara\" se asocia con \"mágica\", \"ánfora\" y \"antorcha\", lo cual encaja con la idea de una lámpara mágica en contextos como \"Las Mil y Una Noches\".\n",
    "* \"sultán\" con \"rey\", \"zahr\", \"visir\", y con lo que parecen nombres propios, como \"schams\" y \"sabur\" (nombres de sultanes).\n",
    "* \"genio\" tiene asociaciones con términos como \"gigantesco\" y \"sucio\", que podrían reflejar características típicas en descripciones de genios en la literatura.\n",
    "* \"alfombra\" con \"fanega\", que es una antigua unidad de medida de superficies, y \"rodilla\" hace sentido por la idea de arrodillarse sobre una alfombra como gesto de reverencia.\n",
    "* \"anillo\" tiene fuertes asociaciones con \"talismánico\", \"engarce\" y \"brazalete\", lo que sugiere que se vincula a conceptos de joyería y magia.\n",
    "pudiendo seguir el análisis con el resto de palabras.\n",
    "\n",
    "En algunos casos, como \"califa\" o el mencionado \"sultán\", aparecen nombres propios de figuras que ocuparon dichos cargos en la época.\n",
    "Finalmente, algunas palabras, aunque cuentan con algunas asociaciones correctas, también parecen presentar bastante ruido.\n",
    "\n",
    "Ambos modelos parecen generar listas con relaciones similares, pero con algunas variaciones en las puntuaciones y las palabras asociadas. En algunos casos, el modelo por páginas tiende a producir términos más literarios o contextuales, mientras que el modelo por líneas parece generar términos más variados y específicos."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Ensayar con una palabra que no está en el vocabulario:\n",
    "w2v_model_by_pages.wv.most_similar(negative=[\"diedaa\"])"
   ],
   "metadata": {
    "id": "mUNf3R6T9OSM",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:38:57.328101Z",
     "start_time": "2025-04-03T03:38:57.243700Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'diedaa' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Ensayar con una palabra que no está en el vocabulario:\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mw2v_model_by_pages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmost_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnegative\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdiedaa\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/posgrado/procesamiento_lenguaje_natural/npl/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001B[39m, in \u001B[36mKeyedVectors.most_similar\u001B[39m\u001B[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[39m\n\u001B[32m    838\u001B[39m         weight[idx] = item[\u001B[32m1\u001B[39m]\n\u001B[32m    840\u001B[39m \u001B[38;5;66;03m# compute the weighted average of all keys\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m mean = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_mean_vector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_normalize\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpost_normalize\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_missing\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    842\u001B[39m all_keys = [\n\u001B[32m    843\u001B[39m     \u001B[38;5;28mself\u001B[39m.get_index(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, _KEY_TYPES) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.has_index_for(key)\n\u001B[32m    844\u001B[39m ]\n\u001B[32m    846\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(topn, \u001B[38;5;28mint\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/posgrado/procesamiento_lenguaje_natural/npl/lib/python3.12/site-packages/gensim/models/keyedvectors.py:518\u001B[39m, in \u001B[36mKeyedVectors.get_mean_vector\u001B[39m\u001B[34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001B[39m\n\u001B[32m    516\u001B[39m         total_weight += \u001B[38;5;28mabs\u001B[39m(weights[idx])\n\u001B[32m    517\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ignore_missing:\n\u001B[32m--> \u001B[39m\u001B[32m518\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mKey \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m not present in vocabulary\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    520\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m total_weight > \u001B[32m0\u001B[39m:\n\u001B[32m    521\u001B[39m     mean = mean / total_weight\n",
      "\u001B[31mKeyError\u001B[39m: \"Key 'diedaa' not present in vocabulary\""
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# el método `get_vector` permite obtener los vectores:\n",
    "vector_palacio = w2v_model_by_pages.wv.get_vector(\"palacio\")\n",
    "print(vector_palacio)"
   ],
   "metadata": {
    "id": "K6zxz8-n92Bf",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:39:01.553153Z",
     "start_time": "2025-04-03T03:39:01.545967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04704217  0.11509896 -0.13339859 -0.4351561   0.18607953 -0.08200072\n",
      "  0.3470688  -0.20336053  0.2543869  -0.11164256 -0.07202603  0.09430519\n",
      "  0.24587733 -0.1092634   0.08288503  0.2113097   0.32497787  0.06026684\n",
      " -0.35083428  0.84050965  0.13715407  0.11795386 -0.15601476 -0.24403554\n",
      "  0.33224124  0.01755694  0.01832088 -0.45076704 -0.4936759  -0.251104\n",
      " -0.21130885 -0.04110062  0.14424354  0.07012124  0.05377048  0.43249896\n",
      "  0.39577925  0.3459927   0.05781981  0.10025173  0.545154   -0.08766089\n",
      "  0.04616233 -0.32763225 -0.33597803 -0.21294837  0.01312517 -0.5174265\n",
      " -0.18831907  0.61813337 -0.10976385  0.06801727 -0.3154639  -0.6018916\n",
      " -0.01201197  0.07679024  0.05033638  0.11472887  0.15622774  0.17118873\n",
      "  0.28579035 -0.15671733 -0.05568857 -0.00524834 -0.34376076  0.30199018\n",
      " -0.08207289  0.22641362  0.05873111 -0.06255728 -0.23417905  0.04295521\n",
      "  0.08272913 -0.05475298  0.36949518  0.23210053 -0.1055919   0.27134272\n",
      "  0.11879314  0.07257785  0.10321553 -0.14943245 -0.01041305 -0.01084467\n",
      " -0.23080632  0.25147796  0.710788    0.40877265 -0.39782497 -0.18234986\n",
      "  0.05283116  0.44873592  0.4143307   0.35504875 -0.0597478  -0.3022814\n",
      "  0.17671505 -0.0576551  -0.15723695  0.08162373 -1.1223409   0.2807377\n",
      "  0.31812546 -0.50652057 -0.4491419   0.21582171 -0.0445729   0.3816739\n",
      " -0.01057059  0.16829813 -0.2729274  -0.12042742  0.3334722  -0.3371811\n",
      " -0.36295268  0.31784138 -0.01936898 -0.04172428 -0.1889921  -0.3499597\n",
      "  0.11250357  0.34402233  0.3060171  -0.4081184   0.20263687  0.1450673\n",
      "  0.07773785  0.14009263  0.35784382 -0.69640374 -0.16061653  0.5081056\n",
      "  0.1162839  -0.17706898  0.0351634   0.11953985 -0.06787456  0.11789121\n",
      "  0.13925537  0.11314403 -0.05121323 -0.3180462   0.08847358  0.07402643\n",
      "  0.10387708 -0.02608466 -0.0460212  -0.48954856 -0.04656455 -0.20445745\n",
      "  0.13185987  0.1079127  -0.45543575  0.13906059 -0.17171966  0.81432074\n",
      " -0.2558029   0.3420325  -0.53730476  0.29344574 -0.03575901  0.07417916\n",
      " -0.17679667  0.4846041  -0.0365206  -0.28156504  0.39722306 -0.16980664\n",
      " -0.32228914  0.19614178 -0.25551233 -0.06292571 -0.17793755  0.14699994\n",
      " -0.2781433   0.364306    0.17365806 -0.2726254  -0.35331538  0.01536456\n",
      " -0.0024089  -0.522599    0.32946298  0.26332098  0.08253098  0.04615536\n",
      "  0.58884484  0.23971565  0.1856632  -0.28700396  0.50285286 -0.56291395\n",
      "  0.02752664  0.42381155  0.08978941 -0.10989326 -0.28925675 -0.6705416\n",
      "  0.35951313  0.14583944  0.08495674 -0.1814554  -0.29670218  0.34048676\n",
      " -0.12503853 -0.06911384  0.10115739  0.08535777  0.26673552  0.31714204\n",
      " -0.14011182 -0.33324662  0.03891765 -0.1195275   0.16112801 -0.27753165\n",
      " -0.24961135 -0.19837911  0.0026611  -0.71551216 -0.01082737 -0.2867979\n",
      "  0.09182488  0.28697434 -0.18633616  0.19605513 -0.10147996 -0.17640743\n",
      " -0.43048456  0.46144274  0.23520334 -0.31493437  0.26998293  0.5659439\n",
      " -0.58007354 -0.13500494  0.09437227 -0.33228526  0.1632198  -0.08189271\n",
      "  0.22139362 -0.27254426 -0.48760027  0.4970899  -0.17830727 -0.02911752\n",
      " -0.11857928 -0.2751876  -0.0427254   0.32685322  0.14486897  0.20044376\n",
      "  0.07970604  0.03848957 -0.19571629 -0.13207144  0.00430674  0.15218414\n",
      "  0.01023252 -0.1476635   0.04992105 -0.14122312  0.3091984  -0.14788087\n",
      " -0.45650646 -0.13462378 -0.00201369 -0.1260331  -0.40786913 -0.06797697\n",
      "  0.01105284 -0.03356712  0.08237926 -0.02133997  0.29694992  0.3803344\n",
      " -0.0376512   0.185536   -0.07138255 -0.1602981  -0.37033665  0.20120928\n",
      "  0.18417338  0.4542043  -0.39408875  0.33726528  0.00826904  0.29982686\n",
      " -0.17421356 -0.2043518   0.24473652  0.02632664 -0.04493462 -0.38482836\n",
      "  0.16378917  0.23002666  0.20934655  0.06084329 -0.31476244 -0.7431903 ]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "# el método `most_similar` también permite comparar a partir de vectores\n",
    "w2v_model_by_pages.wv.most_similar(vector_palacio)"
   ],
   "metadata": {
    "id": "ipjOzoQa9_24",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:39:02.468956Z",
     "start_time": "2025-04-03T03:39:02.432751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('palacio', 1.0),\n",
       " ('celda', 0.3876643478870392),\n",
       " ('pabellón', 0.381332665681839),\n",
       " ('gabinete', 0.3715176284313202),\n",
       " ('gennistán', 0.37010136246681213),\n",
       " ('aparato', 0.36807379126548767),\n",
       " ('cortijo', 0.363415002822876),\n",
       " ('harem', 0.3625940680503845),\n",
       " ('aposento', 0.3618246912956238),\n",
       " ('penetramos', 0.3583143949508667)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 - Visualizar agrupación de vectores\n"
   ],
   "metadata": {
    "id": "kpMfe4Ps-GzJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ],
   "metadata": {
    "id": "ERU9bEO--HLt",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:39:06.768501Z",
     "start_time": "2025-04-03T03:39:06.765804Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Graficar los embedddings en 2D\n",
    "vecs, labels = reduce_dimensions(w2v_model_by_pages)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS, 0], y=vecs[:MAX_WORDS, 1], text=labels[:MAX_WORDS])\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ],
   "metadata": {
    "id": "wMceS7RR-JEN",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:39:52.275901Z",
     "start_time": "2025-04-03T03:39:08.474979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"6fc61518-e394-476c-8720-403cbe46dc98\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"6fc61518-e394-476c-8720-403cbe46dc98\")) {                    Plotly.newPlot(                        \"6fc61518-e394-476c-8720-403cbe46dc98\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"y\",\"de\",\"la\",\"que\",\"a\",\"el\",\"en\",\"se\",\"los\",\"su\",\"al\",\"con\",\"por\",\"no\",\"del\",\"un\",\"las\",\"le\",\"mi\",\"me\",\"una\",\"para\",\"dijo\",\"lo\",\"\\u00a1oh\",\"m\\u00e1s\",\"sus\",\"es\",\"pero\",\"como\",\"te\",\"cuando\",\"tu\",\"ella\",\"entonces\",\"rey\",\"sin\",\"joven\",\"hab\\u00eda\",\"alah\",\"\\u00e9l\",\"este\",\"si\",\"todo\",\"yo\",\"noche\",\"he\",\"ha\",\"tan\",\"contest\\u00f3\",\"hasta\",\"todos\",\"mis\",\"pues\",\"entre\",\"despu\\u00e9s\",\"aqu\\u00ed\",\"\\u00a1y\",\"ya\",\"muy\",\"era\",\"casa\",\"as\\u00ed\",\"sobre\",\"hijo\",\"esta\",\"ojos\",\"d\\u00eda\",\"dos\",\"ni\",\"momento\",\"porque\",\"califa\",\"palabras\",\"lleg\\u00f3\",\"aquel\",\"padre\",\"todas\",\"ma\\u00f1ana\",\"tiempo\",\"qu\\u00e9\",\"cabeza\",\"vi\\u00f3\",\"visir\",\"palacio\",\"luego\",\"cual\",\"estaba\",\"fu\\u00e9\",\"toda\",\"vez\",\"madre\",\"schehrazada\",\"o\",\"cuanto\",\"ver\",\"les\",\"hija\",\"estas\",\"nos\",\"aquella\",\"hizo\",\"mano\",\"tus\",\"bien\",\"m\\u00edo\",\"m\\u00ed\",\"oro\",\"ese\",\"tal\",\"coraz\\u00f3n\",\"esposa\",\"otro\",\"mismo\",\"est\\u00e1\",\"manos\",\"hacer\",\"\\u00a1por\",\"ciudad\",\"quien\",\"nada\",\"vida\",\"hass\\u00e1n\",\"hombre\",\"se\\u00f1or\",\"donde\",\"ti\",\"sult\\u00e1n\",\"desde\",\"tierra\",\"puerta\",\"gran\",\"historia\",\"o\\u00edr\",\"aparecer\",\"mucho\",\"call\\u00f3\",\"ahora\",\"nuestro\",\"hermano\",\"hacia\",\"narraci\\u00f3n\",\"ten\\u00eda\",\"uno\",\"pregunt\\u00f3\",\"otra\",\"alma\",\"cada\",\"exclam\\u00f3\",\"puso\",\"tras\",\"ser\",\"has\",\"t\\u00fa\",\"d\\u00edas\",\"ante\",\"modo\",\"e\",\"\\u00a1pero\",\"discretamente\",\"tres\",\"tambi\\u00e9n\",\"hubo\",\"s\\u00f3lo\",\"fin\",\"agua\",\"princesa\",\"emir\",\"ellos\",\"mil\",\"mujer\",\"\\u00a1no\",\"tanto\",\"hay\",\"esa\",\"jeique\",\"belleza\",\"medio\",\"al\\u00ed\",\"antes\",\"\\u00bfqu\\u00e9\",\"punto\",\"pr\\u00edncipe\",\"saber\",\"cosa\",\"dinares\",\"levant\\u00f3\",\"vieja\",\"mientras\",\"j\\u00f3venes\",\"cogi\\u00f3\",\"camino\",\"m\\u00eda\",\"cosas\",\"son\",\"nuestra\",\"pronto\",\"eso\",\"seguida\",\"soy\"],\"x\":{\"dtype\":\"f4\",\"bdata\":\"33R7QBieSEB5uz5AyohYP+RApj844VdAIQ8YQOZPkUAJrY9Au85eQG+jcUAiGXNAeJgzQFPROr1ni1lAyAIeQGl0gEDyzoxA3rwRQJOlbEAHfixA9JdTP4CUrEBscIY+v9prQHmxqz+W+Y5A3blUv0M8hEFz1thAwQ4DwW0jVEBUkWPBpyc6QMKSVkHrwORAkx69v7NiWEDNdr1BoMwnwJILkUEtBC1B3yirPi6OYkFVUr1AkpehQcHU4UAhVsJBK3I4QRFMq0CjVqBBmMmPQBaSXMDdyIhBuHCuQdkCm0GO7eFA2g6IQcczM0GB0p5A\\u002fegRQas2jUA7gphBrNmiQRdJpUEqr53B6ENoQVf01D8yHm1BRVmrwb+GsUEeE4NBxYYAQWHyjUFlH2Q\\u002fktorQYxPP0E6Ln5A0pGQPrESaEFbr8E\\u002fYi0+QRo8ucAaghdBGU+twGU3qkG9DaZBJtptwe8ZkUGYJptBXfOeQXaLl0G3EbNB5Qx\\u002fwEFzW0GwpVpAtMCNQAfvMEBd2o5BkF0LwGBsXUH8CZtBrno4wbO+dkDU34FB26A6v7N4A0DZlqDBNAQ2QEqLdkFdyFhBdiqXQRabGEERzR0+5K6iQa2qMUHua3JBFFODQSSOFcC8pEJBkwOEve2ff0FhxqJBpcfbP2HDgkFPItfA\\u002fip5wNpG90BO8pJBiEGbQfqg9sD\\u002f2SpBsiK7QNweEkECu1w+LcCbQTLBhUHzBac\\u002fM10RQCA0pUESFKfB6xuzQZc0oEGKrxhBhDCvQF13vb4W\\u002f5lBa82OwKYFsUDjgJNB0P+OQYmWCsDkBL+\\u002fCIq7QBDjKEGRZ5BB82CnQaZrOUEFPYVB7imOwbJQIUGcSnFBeiS0QVDLYD9\\u002fqiBB6kQowWKwHUGb\\u002fdJBjjbCQYzXlMF9\\u002fCZAJ60PPcZWiEE33rg+T6g9QAO91EA4EIrBvYp2wbZwF0GnPCDBD3MZQO+4n0HVQQ5BGi7lwI9hG0CRS5TBc6QfQe9\\u002f60C6G6tB7ieqwbHvMcG2xZZBLqwzwXjvicGKDYRAR75fwSNqqEGJrq6\\u002fh7\\u002fEQRUgWEA=\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"Lq\\u002fZu6VeYz+tfRVAgyyNP\\u002fWGO7+Lq+lAffeSP5sLFT8w55fBgW2wP6D\\u002fmz\\u002fG14Y+XUlOP4hNEj8Op+NA2xT0QPZlY8G8Pi6\\u002f4wqEv7tOC7+M5EZAJN0fwcMcgsDrrbk\\u002f92DVwKM8xT+hQpPByIOrQJWcpEDBH65B6gjgwAW5nEHe7VrB82c8P6mTakCOu5ZBzHXbQXScgT+KzptAv5wbwUENT0HBRmxBxNfdQQTKlkFSfMlBaA2IQDJNzkGGRolAVl2RQZbZg8CpZAZALgybwUfD2cG\\u002fGohBt7fewJu7acBiWs5BBeeTQe3Ym0Ercr9B\\u002fNieQWk8o0EyEjFBPAzxPxVjZ0GVozzBL7LuwCY4mUExpr7BinU\\u002fwXrt3kCVYp9BJFxeQZrXi8BM2qtBrMaAQX7kgUGLtWTBIugYwPSqZkG\\u002fA\\u002frAPjCpQX3KtkFezXhBxIF7QYPfyEC1PTRB7SCFQUlioEH06uFAZGknQM2ltr+3KeZANKHAQTRCmkFzFhk\\u002fdoY5v5QXuj+AQ4vAjyNawQ9CgEGSWeDAoyGxQS0UgcG9Q21BTWy6wNKO0kGwyBVBAQDyQNiwakHzuqU\\u002ffbgnQWGUnEFepNJBg\\u002fNwPvL+FsF9t5xBokyBwB1fj0Hc34pBgHQBP4DmhEGH8VRBxPVMQRVqeEHqDKlBKs+GwM4CmUGNUUhBlV9zQS39kEFlzIhBLdoHQaNmVUGeiZBBGg0OQSW3Q0CfGcS+CIaEv9BHZkEQbwzBbmblQIAr+0DnguLBYAWGwGCwzUECNFVBcFzNQeGfgcChwZ7AbdrKwFwHyEG9Q+7A4ViwQeUyr8E1yBxAY8flQLl6j8EF6KRAbP2xwfVIwMEkb6NBSyaTQHlBz0HWiK9BzpllQTjmTkE8RzJBFjixP3+PbcFV48U\\u002f6x3VPf2NhEG9ahXBpfMmQJEmX0FyTaNAyrqZQYf\\u002fg0FM\\u002fijBniDkwEFbSED9yIdB5gYxwCOLnUDPnWzB2EUUwMBP90CgHyVAQev5v4dV6UD210JBpwhcwfXOhsGRzHjBsr5kwRrLGEFJAtJBqHcJQSVnKEE=\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"bgcolor\":\"rgb(17,17,17)\",\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"rgb(17,17,17)\",\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"subunitcolor\":\"#506784\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"rgb(17,17,17)\"},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"borderwidth\":1,\"bordercolor\":\"rgb(17,17,17)\",\"tickwidth\":0},\"mapbox\":{\"style\":\"dark\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6fc61518-e394-476c-8720-403cbe46dc98');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "# Graficar los embedddings en 3D\n",
    "vecs, labels = reduce_dimensions(w2v_model_by_pages, 3)\n",
    "\n",
    "fig = px.scatter_3d(x=vecs[:MAX_WORDS, 0], y=vecs[:MAX_WORDS, 1], z=vecs[:MAX_WORDS, 2],text=labels[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ],
   "metadata": {
    "id": "EVe6isc4-Lh8",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:42:15.414539Z",
     "start_time": "2025-04-03T03:39:52.277094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"49e08ca2-abdf-4986-a7df-3b9253741d26\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"49e08ca2-abdf-4986-a7df-3b9253741d26\")) {                    Plotly.newPlot(                        \"49e08ca2-abdf-4986-a7df-3b9253741d26\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003ez=%{z}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":2},\"mode\":\"markers+text\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"text\":[\"y\",\"de\",\"la\",\"que\",\"a\",\"el\",\"en\",\"se\",\"los\",\"su\",\"al\",\"con\",\"por\",\"no\",\"del\",\"un\",\"las\",\"le\",\"mi\",\"me\",\"una\",\"para\",\"dijo\",\"lo\",\"\\u00a1oh\",\"m\\u00e1s\",\"sus\",\"es\",\"pero\",\"como\",\"te\",\"cuando\",\"tu\",\"ella\",\"entonces\",\"rey\",\"sin\",\"joven\",\"hab\\u00eda\",\"alah\",\"\\u00e9l\",\"este\",\"si\",\"todo\",\"yo\",\"noche\",\"he\",\"ha\",\"tan\",\"contest\\u00f3\",\"hasta\",\"todos\",\"mis\",\"pues\",\"entre\",\"despu\\u00e9s\",\"aqu\\u00ed\",\"\\u00a1y\",\"ya\",\"muy\",\"era\",\"casa\",\"as\\u00ed\",\"sobre\",\"hijo\",\"esta\",\"ojos\",\"d\\u00eda\",\"dos\",\"ni\",\"momento\",\"porque\",\"califa\",\"palabras\",\"lleg\\u00f3\",\"aquel\",\"padre\",\"todas\",\"ma\\u00f1ana\",\"tiempo\",\"qu\\u00e9\",\"cabeza\",\"vi\\u00f3\",\"visir\",\"palacio\",\"luego\",\"cual\",\"estaba\",\"fu\\u00e9\",\"toda\",\"vez\",\"madre\",\"schehrazada\",\"o\",\"cuanto\",\"ver\",\"les\",\"hija\",\"estas\",\"nos\",\"aquella\",\"hizo\",\"mano\",\"tus\",\"bien\",\"m\\u00edo\",\"m\\u00ed\",\"oro\",\"ese\",\"tal\",\"coraz\\u00f3n\",\"esposa\",\"otro\",\"mismo\",\"est\\u00e1\",\"manos\",\"hacer\",\"\\u00a1por\",\"ciudad\",\"quien\",\"nada\",\"vida\",\"hass\\u00e1n\",\"hombre\",\"se\\u00f1or\",\"donde\",\"ti\",\"sult\\u00e1n\",\"desde\",\"tierra\",\"puerta\",\"gran\",\"historia\",\"o\\u00edr\",\"aparecer\",\"mucho\",\"call\\u00f3\",\"ahora\",\"nuestro\",\"hermano\",\"hacia\",\"narraci\\u00f3n\",\"ten\\u00eda\",\"uno\",\"pregunt\\u00f3\",\"otra\",\"alma\",\"cada\",\"exclam\\u00f3\",\"puso\",\"tras\",\"ser\",\"has\",\"t\\u00fa\",\"d\\u00edas\",\"ante\",\"modo\",\"e\",\"\\u00a1pero\",\"discretamente\",\"tres\",\"tambi\\u00e9n\",\"hubo\",\"s\\u00f3lo\",\"fin\",\"agua\",\"princesa\",\"emir\",\"ellos\",\"mil\",\"mujer\",\"\\u00a1no\",\"tanto\",\"hay\",\"esa\",\"jeique\",\"belleza\",\"medio\",\"al\\u00ed\",\"antes\",\"\\u00bfqu\\u00e9\",\"punto\",\"pr\\u00edncipe\",\"saber\",\"cosa\",\"dinares\",\"levant\\u00f3\",\"vieja\",\"mientras\",\"j\\u00f3venes\",\"cogi\\u00f3\",\"camino\",\"m\\u00eda\",\"cosas\",\"son\",\"nuestra\",\"pronto\",\"eso\",\"seguida\",\"soy\"],\"x\":{\"dtype\":\"f4\",\"bdata\":\"99gMRRydWUH0vFy+V5OAQ9BGE0ISbLrDNeT8QZUflMFJh4vCnWu7vwWSecE+FIvBwvMQPrymM0BvRWLBY8MbQZ0g08AEA5zBefW0Pl3goMExzWE\\u002ferNGQYnD+MEUsWZBBJDIwQa9uL\\u002fOgXfBkCK6QAfdw0G0BIxCEia9QAaie0KcjtU\\u002f2WxVwQBSF0LR0FfBgBtYQjHF3MFEK6VB4j0wQO7j\\u002fUHCkEBCLB\\u002fYQda+S0Kj8FFAFMsaQsvRJEIYZxJAQV4BQnSc88EfTlxCetFTQam3gkHHvbrBLZJzQpB260DhxyFCC91sQlWIxUDIjmHCqihcQdcXg0IjeD5CLCh6Qg5WNsHhmH9CDxhJwo8s4UFI6O9Ab3x9QteeKUKpM5m\\u002fE9EFwrE7tME78g1CsUK4PrhrlsCD5ZpAkcluwZjkKcAkzSHCcdw5QpLdSUGwfpjBNv1LQsHGgUKWoBJCR+vcQaOPMEKVEKBBAFVvQnVq6D63GO7AWoFwQjFGA0CzhrDBXQ2kwWRzG7+PuDLCvMYQQnSJCUJnTU1C2cskQgMHp8FYlk7BK+AbwokRAUIv6edAxPUXQcZYLcD3lB\\u002fCANKfwChUhUIk0QNCoLTUwMUrc8HK74JCgH88QSM1KELVMufA\\u002fJCHQE8ae0K6MVzC4i5gQuYT6z95RGFCNmqDQoDWm8D9cixCmpBUQj+qPkLvuORBmRTGwZnjJEJOPbpB6+6AwjjdGMJIvkBCV1UbQUk9OsIxzWtCT5ChwMchIkJ3CQlCmMYIwkusR0KbQabADyZwQguK6MFUslJCbGYzQudKX0KkcSfA4LRjQVdHi0FMm29CjpGEQsJdH0KR5kBCnlIkwgx3CEJLOnxB5Cv2PrMygEJLbzpBW0XmwLqUCMEsyQ\\u002fBksaFQtyLpEEcSLPB4ZO2QERK7UBATG5Ci2HmQZ1GBsLuQk\\u002fCyPMeQrITa8K+T4VC7xbpwR1PekJNuD3CmpPpP0lO4sGCEKpBzOoGwaFpgz4e5iZCGXkBwbsd8kFHUmFCPWEbwlirWMGYc3TBKUWIQaryskG2Bw9BeDI4Qus8MsE=\"},\"y\":{\"dtype\":\"f4\",\"bdata\":\"r5VNQ3ZvHUEebb6\\u002fn3zLw8cdoUGjFnbE78xtwN+zkj8hM9FCUf6oQYN1zT9Ghp9AJ1BaQa75rkA1c5hAnu+5waocE0LILz8515ylQap+0L\\u002ft5M\\u002fBzWz8QRQ1JEGFo8rB51nSQAJBBUGPfnNCAZN3wR9oysHoGC8\\u002fgCVfwVd8YEAG8bVBdrjYwbWwYMHVHoRBD1OMwa\\u002fT+kGHXZPBctVMQT5\\u002fwkEYubVBAFmfwQcYzcEAJN7BxDk\\u002fQq2TFMIkG5jAokFQQglKK0FDnyTBwFYmQsD+YkIIRDHCG+gGQnkfP8G6oxrCNRMAwlTjs8FbuRFBD2jVQRtJD0FOSKHBYC9cv9rsv8Dx8\\u002fzAOvQjvqyqM0In\\u002fGhCzwyBQY\\u002fPwEHigP7BfqDeQHr7wkFmMhtCqWOTQazxaMHY5lZCnargQXMQccHSX+PACYs1QjakCkF62r\\u002fAIymgwSExmsHQjEbCjxVawq8Xs8BhsWJChtCjQdR4QUCGaT1B5N48QaDAHcJGyiHB2qEDQD9Bv8Ef3GdA3xnqQTtyo0Eico1BRqNWwumkbUJsmkfCuRnTwUF6g0FjKxTCoYCmweu7KkIFaiVCQh5gwXLysMFpFJrBvFU7wjFUx0F7NhVAE+bgwbEWAELtSKXBaw0CQZHtWkE5Rp3A7F4hwsQx28EOzPxAELOxwBZ73cH7S7JBSdc6QmUpIcAH79DBXkQEQAvzOEIxHi1CFPhqQSp1DkKw8A7CQIqJQTj0BMLXKQxCuRYhQereAcJdBClBrEoQQchQOEI9LBBC671DwXOzEEHWwOFBt\\u002fEbQgVj08Heiui\\u002fz82xwf3hMEIfxtxBEsRKwdNKO0GyYiDCQTkQQgrM4kHcYmnBbMKDQjJWmcH+gDzB6rEkwg8GmsG8IJM+pGEtQS4pcMFwnI7Bpb+GQKAv4sEmhDxBZJ0cwULntkC2691A4Eaewd+hqcFnKpnA92UFwS4fM8FieEVCZZ2LwXkKJsE8tIjBe9HiwZAVT8FJ\\u002fDLBbVAHwuE1D8IorLJBdo1KQXfgJkLgITFCgtgNQup9FUIRtKrBjqogwg46c8A=\"},\"z\":{\"dtype\":\"f4\",\"bdata\":\"fXkoRGACWb6kzfFBrqyfwWLOkMOwqevBz\\u002fy5QfOwIUE8kijC6LkMQpqF4kDxGCtBDLrJQeae20Eg9v9ATyL0QRnrwcEqWrlArXgRQgWC7EBMsiRCc608QBi2DcFTPAhCk6mFwfsrcELiXvHBZur8QQfdSEKRRVdBWtjCQJj\\u002f6cF71g5CL19fQsC3KEIe+QXCFu66QcgFYUJzBixCM89HwRnQb0KpFDxCukpDQk4MrT22O4FCLQyZQYp6E0L0yIXC+osXQhj9McHwHSdCtkdawo4MDsKD3kLC49xWQf\\u002fMjEIJSRtC+XiYQRstdkJstcBBmjWDQl6t+EBH\\u002fzlCGzflQY8chMF+e+lBXLH6wB3LCEJvcyLCAumRQUq8O0JciG9CAmIqwvbst8FR\\u002fOBBL3SNQrlcBMFIh7vBO1u8QXHqi0Kvf4bBrUblQcZyhkLQL2TCUIWFQcH1u8BWuslB2zamQXknN0LCegVCj7EBQi\\u002f7jcJyvX9CPYkPQkIpaUIS9oRCTuu+QHxZcEK6U0rC2dVaQa1lV0KCVALC6y+UwDRO38EIyDhCQFZBQmjza0IRNEzB+F3mQeahUUJR6aRAwFV8Qkml60AcvWpCYuhFQpkBwcFw\\u002fpBBATxOQndRu8AnG0xBGajYQexq40FAwIFBoY9DwZCcXkJwhcpBvo2owXT0f0J1DiJC8NvOQMQH2kGbRmRCfj1NQppjBkKaJ+9BbMyTQag1EkJPEQlCvWhtQjUtFEJnLa9Bk0CEQqQ+MUIkCGXCQNAkwUkprkH7O0ZC+F4TQg5cA8FTnJ\\u002fB3S0fQkJL+kEZXeNAap9WQm44ksBmpNZBvoyXQY1fxsFrCuBBud8NQrddUcLDwYXCanvbweL0ecG3r4FCvBUzQdD4VkJrBH\\u002fCfqyzQacNKMJaPF9CX9ngQX0faEId7LJBqN5UQmesEcJ8dCtCcwUxQlu5K8EKvKa+lcfswBGASEEngkbBjJxxQi+2PEKluy7CqMumQTq\\u002fFkJLX2JCnLE1wg5GFME1ABVB062WwSV2DsKYnZ\\u002fBQKowQtpcX0KSnWBC8weqQZlHosE=\"},\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"bgcolor\":\"rgb(17,17,17)\",\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"rgb(17,17,17)\",\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"subunitcolor\":\"#506784\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"rgb(17,17,17)\"},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"borderwidth\":1,\"bordercolor\":\"rgb(17,17,17)\",\"tickwidth\":0},\"mapbox\":{\"style\":\"dark\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('49e08ca2-abdf-4986-a7df-3b9253741d26');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "# También se pueden guardar los vectores y labels como tsv para graficar en\n",
    "# http://projector.tensorflow.org/\n",
    "\n",
    "vectors = np.asarray(w2v_model_by_pages.wv.vectors)\n",
    "labels = list(w2v_model_by_pages.wv.index_to_key)\n",
    "\n",
    "np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
    "\n",
    "with open(\"labels.tsv\", \"w\") as fp:\n",
    "    for item in labels:\n",
    "        fp.write(\"%s\\n\" % item)"
   ],
   "metadata": {
    "id": "L6WxUwJq-NGC",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:42:17.077789Z",
     "start_time": "2025-04-03T03:42:15.415678Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6 - Tests de analogías"
   ],
   "metadata": {
    "id": "J0VO0GcS-UKX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definir la función de analogía\n",
    "def analogy(model, word_a, word_b, word_c, topn=10):\n",
    "    \"\"\"\n",
    "    Encuentra la palabra D que complete la analogía: A es a B como C es a ?\n",
    "\n",
    "    Parámetros:\n",
    "    - model: modelo Word2Vec entrenado\n",
    "    - word_a, word_b, word_c: palabras en la analogía\n",
    "    - topn: cantidad de palabras más similares a mostrar\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de palabras similares con sus puntuaciones\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=topn)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e}. Alguna palabra no está en el vocabulario.\")\n",
    "        return None"
   ],
   "metadata": {
    "id": "R76bhd5F-YAO",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:42:17.081228Z",
     "start_time": "2025-04-03T03:42:17.079010Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "id": "oR9rCGVMnmza",
    "outputId": "bb7e4145-82ac-4104-89d4-d39aa6c7e8ee",
    "ExecuteTime": {
     "end_time": "2025-04-03T03:42:17.123845Z",
     "start_time": "2025-04-03T03:42:17.081896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analogy_compare_models(model_by_pages, model_by_lines, word_a, word_b, word_c, topn=10):\n",
    "    \"\"\"\n",
    "    Compara los resultados de la analogía en dos modelos de Word2Vec.\n",
    "\n",
    "    Parámetros:\n",
    "    - model_by_pages: modelo Word2Vec entrenado por páginas\n",
    "    - model_by_lines: modelo Word2Vec entrenado por líneas\n",
    "    - word_a, word_b, word_c: palabras en la analogía (A es a B como C es a ?)\n",
    "    - topn: cantidad de palabras más similares a mostrar\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener resultados de analogía para ambos modelos\n",
    "    similar_by_pages = analogy(model_by_pages, word_a, word_b, word_c, topn)\n",
    "    similar_by_lines = analogy(model_by_lines, word_a, word_b, word_c, topn)\n",
    "\n",
    "    # Manejar errores si alguna palabra no está en el vocabulario\n",
    "    if similar_by_pages is None or similar_by_lines is None:\n",
    "        return\n",
    "\n",
    "    # Convertir a listas formateadas\n",
    "    words_pages = [f\"{w[0]} ({w[1]:.2f})\" for w in similar_by_pages]\n",
    "    words_lines = [f\"{w[0]} ({w[1]:.2f})\" for w in similar_by_lines]\n",
    "\n",
    "    # Asegurar que ambas listas tengan el mismo tamaño\n",
    "    max_len = max(len(words_pages), len(words_lines))\n",
    "    words_pages += [\"\"] * (max_len - len(words_pages))\n",
    "    words_lines += [\"\"] * (max_len - len(words_lines))\n",
    "\n",
    "    # Crear tabla formateada\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Comparación para la analogía: '{word_a}' es a '{word_b}' lo que '{word_c}' es a:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"   Model By Pages   |  Model By Lines \")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for w1, w2 in zip(words_pages, words_lines):\n",
    "        print(f\" {w1.ljust(20)} | {w2}\")\n",
    "\n",
    "    print(\"-\" * 60)  # Línea final separadora\n",
    "\n",
    "# Lista de analogías a comparar\n",
    "analogies = [\n",
    "    (\"princesa\", \"príncipe\", \"reina\"),\n",
    "    (\"jeque\", \"tribu\", \"califa\"),\n",
    "    (\"anillo\", \"engarce\", \"palacio\"),\n",
    "    (\"sultán\", \"palacio\", \"mercader\"),\n",
    "]\n",
    "\n",
    "# Generar comparación para cada analogía\n",
    "for word_a, word_b, word_c in analogies:\n",
    "    analogy_compare_models(w2v_model_by_pages, w2v_model_by_lines, word_a, word_b, word_c)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Comparación para la analogía: 'princesa' es a 'príncipe' lo que 'reina' es a:\n",
      "============================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "------------------------------------------------------------\n",
      " hossein (0.29)       | hossein (0.29)\n",
      " oriunda (0.26)       | rey (0.28)\n",
      " rey (0.25)           | persa (0.27)\n",
      " desdichado (0.25)    | sabio (0.26)\n",
      " sabio (0.24)         | yamlika (0.26)\n",
      " visirato (0.24)      | coquetería (0.26)\n",
      " desmayada (0.24)     | diadema (0.25)\n",
      " yamaní (0.24)        | mahmud (0.25)\n",
      " alargó (0.24)        | sillón (0.25)\n",
      " llamará (0.24)       | calamidades (0.25)\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Comparación para la analogía: 'jeque' es a 'tribu' lo que 'califa' es a:\n",
      "============================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "------------------------------------------------------------\n",
      " califal (0.28)       | erguí (0.27)\n",
      " afrenta (0.27)       | billah (0.26)\n",
      " dejado (0.25)        | cordeleros (0.26)\n",
      " servidoras (0.24)    | digas (0.26)\n",
      " prerrogativas (0.24) | avisarme (0.26)\n",
      " impulsos (0.24)      | conservando (0.25)\n",
      " hacerle (0.24)       | afrenta (0.25)\n",
      " pasamos (0.24)       | ¡lejos (0.25)\n",
      " camelleros (0.24)    | continua (0.25)\n",
      " agha (0.24)          | producen (0.25)\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Comparación para la analogía: 'anillo' es a 'engarce' lo que 'palacio' es a:\n",
      "============================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "------------------------------------------------------------\n",
      " celda (0.41)         | celda (0.37)\n",
      " espacioso (0.37)     | pórtico (0.36)\n",
      " pórtico (0.37)       | cortijo (0.34)\n",
      " cortijo (0.36)       | dejándolo (0.34)\n",
      " aposento (0.36)      | penetramos (0.34)\n",
      " ladrillos (0.35)     | trablús (0.33)\n",
      " doradas (0.35)       | ajam (0.33)\n",
      " timbales (0.35)      | espaciosa (0.33)\n",
      " silbido (0.35)       | mirándose (0.33)\n",
      " castillo (0.34)      | planeta (0.33)\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Comparación para la analogía: 'sultán' es a 'palacio' lo que 'mercader' es a:\n",
      "============================================================\n",
      "   Model By Pages   |  Model By Lines \n",
      "------------------------------------------------------------\n",
      " tienda (0.31)        | mercaderes (0.28)\n",
      " casa (0.29)          | sucederme (0.27)\n",
      " zoco (0.28)          | bordo (0.27)\n",
      " estrado (0.28)       | estrado (0.27)\n",
      " callejón (0.26)      | desván (0.26)\n",
      " pasillo (0.26)       | alcoba (0.26)\n",
      " cortijo (0.26)       | maghreb (0.25)\n",
      " labrada (0.26)       | pasillo (0.25)\n",
      " mercaderes (0.25)    | derredor (0.25)\n",
      " papeletas (0.25)     | aspecto (0.24)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "id": "WUT13eB5nmza"
   },
   "cell_type": "markdown",
   "source": [
    "#### Análisis de Analogías\n",
    "\n",
    "1. \"princesa\" es a \"príncipe\" lo que \"reina\" es a:\n",
    "\n",
    "* El modelo basado en páginas tiene \"rey\" como una de las respuestas más cercanas, lo que es lógico dada la relación monárquica.\n",
    "* El modelo basado en líneas sugiere \"héroe\" y \"hossein\", posiblemente reflejando asociaciones narrativas dentro del texto.\n",
    "* Otras respuestas como \"pastelero\" o \"porteros\" en el modelo por páginas parecen menos relacionadas semánticamente con la analogía planteada.\n",
    "\n",
    "2. \"jeque\" es a \"tribu\" lo que \"califa\" es a:\n",
    "\n",
    "* En el modelo basado en páginas, \"califal\" aparece como la mejor opción, lo que tiene sentido, dado que \"califa\" está relacionado con \"califato\".\n",
    "* El modelo basado en líneas genera respuestas menos relacionadas, como \"castigarme\" o \"avisarme\".\n",
    "* La presencia de \"emisarios\" en el modelo por líneas podría estar relacionada con el contexto histórico de los califas y su diplomacia.\n",
    "\n",
    "3. \"anillo\" es a \"engarce\" lo que \"palacio\" es a:\n",
    "\n",
    "* El modelo basado en líneas sugiere \"pórtico\", \"vestíbulo\" y \"harem\", palabras asociadas a palacios.\n",
    "* El modelo basado en páginas genera respuestas como \"penetramos\", \"celda\" y \"gradas\", lo que también está relacionado con estructuras arquitectónicas.\n",
    "* La presencia de \"astrolabio\" en ambas listas sugiere que en la narrativa el término está asociado a palacios.\n",
    "\n",
    "4. \"sultán\" es a \"palacio\" lo que \"mercader es a:\n",
    "\n",
    "* \"tienda\" aparece como la mejor respuesta en el modelo basado en páginas, lo que es una elección semánticamente coherente.\n",
    "* En el modelo basado en líneas, aparecen palabras menos esperadas como \"caballo\" y \"pasillo\".\n",
    "* \"Khan\" en el modelo basado en líneas podría referirse a posadas orientales, lo que tiene cierto grado de relación con mercaderes.\n",
    "\n",
    "#### Conclusión\n",
    "\n",
    "Los resultados muestran que el modelo basado en páginas tiende a producir respuestas más alineadas con el significado semántico esperado en cada analogía. En cambio, el modelo basado en líneas genera algunas respuestas coherentes, pero también incorpora términos menos esperados, lo que podría deberse a asociaciones específicas del contexto narrativo en el que aparecen las palabras. Esto sugiere que la segmentación del texto influye significativamente en las asociaciones semánticas que los modelos pueden capturar."
   ]
  }
 ]
}
